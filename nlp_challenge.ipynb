{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# read csv\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# observe first five instances of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe an example review to see if textual preprocessing is needed\n",
    "example_review = data.iloc[1, 0]\n",
    "example_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bs4 to access BeautifulSoup, which has a html parser\n",
    "from bs4 import BeautifulSoup\n",
    "# load re for substitute function\n",
    "import re\n",
    "\n",
    "def parse_review(text):\n",
    "    # functiion for parsing html string\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    parsed_text = soup.get_text()\n",
    "    return str(parsed_text)\n",
    "\n",
    "def clean_text(text):\n",
    "    # function for cleaning text\n",
    "\n",
    "    # parse text\n",
    "    parsed_text = parse_review(text)\n",
    "\n",
    "    # substitute symbology with an empty string\n",
    "    cleaned_text = re.sub(\"\\[[^]]*\\]'-\", '', parsed_text)\n",
    "\n",
    "    # lowercase all text\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whan/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# apply cleaning to the review column in data\n",
    "\n",
    "data['review'] = data['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn module for splitting data into 70:15:15 proportions and binarizing labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# X is our input and y is our target\n",
    "X = data['review'].values\n",
    "y = data['sentiment'].values\n",
    "\n",
    "# binarize labels\n",
    "binarized_labels = label_binarize(y, classes = ['negative', 'positive'])\n",
    "\n",
    "# reshape binarized labels array into a 1 dimensional array\n",
    "binarized_labels = binarized_labels.squeeze(1)\n",
    "\n",
    "\n",
    "# Split data into train validation and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, binarized_labels, shuffle = True, test_size=0.3, random_state=2)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, shuffle = True, test_size = 0.5, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how many positive and negative labels\n",
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227.11458\n"
     ]
    }
   ],
   "source": [
    "# get average token length\n",
    "\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    token_lengths.append(len(X[i].split()))\n",
    "    \n",
    "average_token_length = sum(token_lengths)/len(token_lengths)\n",
    "\n",
    "print(average_token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whan/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "max_length = 250\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "number_of_classes = 2\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "pretrained_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# garbage collect and empty cuda cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set device to gpu or cpu, whichever is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules to create PyTorch Dataset and PyTorch DataLoader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Create PyTorch Dataset class\n",
    "class TextSentimentDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments, tokenizer, max_length, device):\n",
    "        self.reviews = reviews\n",
    "        self.sentiments = sentiments\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentiments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        sentiment = self.sentiments[idx]\n",
    "        \n",
    "        # Tokenize reviews and retrieve input_ids for model\n",
    "        inputs = self.tokenizer(review, \n",
    "                                max_length = self.max_length,\n",
    "                                add_special_tokens = True,\n",
    "                                return_attention_mask = True, \n",
    "                                padding = 'max_length', \n",
    "                                truncation = True, \n",
    "                                return_tensors = 'pt').to(self.device)\n",
    "        input_ids = inputs.input_ids\n",
    "        attention_mask = inputs.attention_mask\n",
    "        sentiment = torch.tensor(sentiment, dtype=torch.long)\n",
    "        \n",
    "        return input_ids, attention_mask, sentiment\n",
    "    \n",
    "\n",
    "def create_dataloader(X, y, batch_size, tokenizer, max_length, device):\n",
    "    # function for creating PyTorch DataLoader\n",
    "\n",
    "    pytorch_dataset = TextSentimentDataset(\n",
    "        reviews = X,\n",
    "        sentiments = y,\n",
    "        tokenizer = tokenizer,\n",
    "        max_length = max_length,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    pytorch_dataloader = DataLoader(\n",
    "        dataset = pytorch_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    return pytorch_dataloader, pytorch_dataset\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders for train, validation, test sets\n",
    "\n",
    "train_loader, train_dataset = create_dataloader(X_train, y_train, batch_size, tokenizer, max_length, device)\n",
    "val_loader, val_dataset = create_dataloader(X_val, y_val, batch_size, tokenizer, max_length, device)\n",
    "test_loader, test_dataset = create_dataloader(X_test, y_test, batch_size, tokenizer, max_length, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch modules for creating model and loss criterion\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def calculate_loss(pred, label):\n",
    "    #function for calculating loss and returning number of correct predictions\n",
    "    loss = F.cross_entropy(pred, label, reduction = 'sum')\n",
    "    pred = pred.max(1)[1]\n",
    "    number_correct = pred.eq(label).sum().item()\n",
    "\n",
    "    return loss, number_correct\n",
    "\n",
    "class SentimentAnalysis(nn.Module):\n",
    "    # Pytorch class for creating models\n",
    "    def __init__(self, number_of_classes, pretrained_model):\n",
    "        super(SentimentAnalysis, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        # Softmax activation function to get probabilities \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        out = self.pretrained_model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels = labels\n",
    "        )\n",
    "        logits = out.logits\n",
    "        loss = out.loss\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy module\n",
    "import numpy as np\n",
    "\n",
    "class EarlyStopper:\n",
    "    # stops the training process if the model does not improve validation loss\n",
    "    def __init__(self, patience, min_delta):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = SentimentAnalysis(\n",
    "    number_of_classes = number_of_classes,\n",
    "    pretrained_model = pretrained_model\n",
    ").to(device)\n",
    "\n",
    "# instantiate Adam optimizer\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#instantiate early stopping policy\n",
    "early_stopping = EarlyStopper(patience=3, min_delta=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def calculate_statistic(cm, class_num):\n",
    "    # function for calculating precision, recall, and f1 score from confusion matrix\n",
    "    total_pred = cm.sum(0)\n",
    "    total_true = cm.sum(1)\n",
    "\n",
    "    pre_i = [cm[i, i] / total_pred[i] for i in range(class_num)]\n",
    "    rec_i = [cm[i, i] / total_true[i] for i in range(class_num)]\n",
    "    F1_i = [2 * pre_i[i] * rec_i[i] / (pre_i[i] + rec_i[i]) for i in range(class_num)]\n",
    "\n",
    "    pre_i = np.array(pre_i)\n",
    "    rec_i = np.array(rec_i)\n",
    "    F1_i = np.array(F1_i)\n",
    "    pre_i[np.isnan(pre_i)] = 0\n",
    "    rec_i[np.isnan(rec_i)] = 0\n",
    "    F1_i[np.isnan(F1_i)] = 0\n",
    "\n",
    "    return sum(list(pre_i))/len(list(pre_i)), sum(list(rec_i))/len(list(rec_i)), sum(list(F1_i))/len(list(F1_i))\n",
    "\n",
    "\n",
    "# functions for training, evaluating, and testing\n",
    "\n",
    "def train(train_loader, device, model, optimizer, total_num):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0    \n",
    "    for batch in tqdm(train_loader, desc='- (Training)  '): \n",
    "\n",
    "        input_ids, attention_mask, label = map(lambda x: x.to(device), batch)\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = input_ids.squeeze(1)\n",
    "        attention_mask = attention_mask.squeeze(1)\n",
    "        pred, loss = model(input_ids, attention_mask, labels = label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_loss = total_loss / total_num\n",
    "    train_acc = total_correct / total_num\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def validate(val_loader, device, model, total_num):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='- (Validating)  '): \n",
    "\n",
    "            input_ids, attention_mask, label = map(lambda x: x.to(device), batch)\n",
    "            input_ids = input_ids.squeeze(1)\n",
    "            pred, loss = model(input_ids, attention_mask, labels = label)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    val_loss = total_loss / total_num\n",
    "    val_acc = total_correct / total_num\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test(test_loader, device, model, total_num):\n",
    "    all_labels = []\n",
    "    all_res = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='- (Testing)  '): \n",
    "            \n",
    "            input_ids, attention_mask, label = map(lambda x: x.to(device), batch)\n",
    "            input_ids = input_ids.squeeze(1)\n",
    "            pred, loss = model(input_ids, attention_mask, labels = label)\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_res.extend(pred.max(1)[1].cpu().numpy())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    test_loss = total_loss / total_num\n",
    "    test_acc = total_correct / total_num\n",
    "\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Test Accuracy: {test_acc}')\n",
    "    \n",
    "    # create confusion matrix from labels and predictions\n",
    "    cm = confusion_matrix(all_labels, all_res)\n",
    "    print(f'Test confusion matrix: {cm}')\n",
    "    Precision, Recall, F1 = calculate_statistic(cm, number_of_classes)\n",
    "    print(f'Test Precision: {Precision}')\n",
    "    print(f'Test Recall: {Recall}')\n",
    "    print(f'Test F1: {F1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 0 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 2188/2188 [11:28<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6957314171346494\n",
      "Train Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 469/469 [01:08<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6940907792750198\n",
      "Val Accuracy: 0.0\n",
      "    - [Info] The checkpoint file has been updated.\n",
      "[ Epoch 1 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 2188/2188 [11:28<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6950586172028477\n",
      "Train Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 469/469 [01:08<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6935922893633975\n",
      "Val Accuracy: 0.0\n",
      "    - [Info] The checkpoint file has been updated.\n",
      "[ Epoch 2 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 2188/2188 [11:28<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6945953287688208\n",
      "Train Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 469/469 [01:08<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.69355056344319\n",
      "Val Accuracy: 0.0\n",
      "    - [Info] The checkpoint file has been updated.\n",
      "[ Epoch 3 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 2188/2188 [11:27<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6942718693172691\n",
      "Train Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 469/469 [01:08<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6939905700144737\n",
      "Val Accuracy: 0.0\n",
      "[ Epoch 4 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 2188/2188 [11:26<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6941180312546341\n",
      "Train Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 469/469 [01:08<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.693086196618802\n",
      "Val Accuracy: 0.0\n",
      "    - [Info] The checkpoint file has been updated.\n",
      "[ Epoch 5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 2188/2188 [11:29<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6940108839238583\n",
      "Train Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 469/469 [01:08<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931016701879278\n",
      "Val Accuracy: 0.0\n",
      "[ Epoch 6 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  :  38%|███▊      | 836/2188 [04:24<07:07,  3.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, epochs):\n\u001b[1;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[ Epoch\u001b[39m\u001b[39m'\u001b[39m, epoch, \u001b[39m'\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(train_loader, device, model, optimizer, \u001b[39mlen\u001b[39;49m(train_loader))\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, device, model, optimizer, total_num)\u001b[0m\n\u001b[1;32m     36\u001b[0m attention_mask \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m pred, loss \u001b[39m=\u001b[39m model(input_ids, attention_mask, labels \u001b[39m=\u001b[39m label)\n\u001b[0;32m---> 39\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     41\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import module for plotting learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_epoch = 0\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "all_epochs = []\n",
    "\n",
    "# training and validating loop\n",
    "for epoch in range(0, epochs):\n",
    "    print('[ Epoch', epoch, ']')\n",
    "\n",
    "    train_loss, train_acc = train(train_loader, device, model, optimizer, len(train_loader))\n",
    "    print(f'Train Loss: {train_loss}')\n",
    "    print(f'Train Accuracy: {train_acc}')\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_loss, val_acc = validate(val_loader, device, model, len(val_loader))\n",
    "    print(f'Val Loss: {val_loss}')\n",
    "    print(f'Val Accuracy: {val_acc}')\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # save best model checkpoint\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "    checkpoint = {\n",
    "        'model': model_state_dict,\n",
    "        'config_file': 'config',\n",
    "        'epoch': epoch}\n",
    "\n",
    "    if val_loss <= min(val_losses):\n",
    "        torch.save(checkpoint, 'checkpoint_best.pth')\n",
    "        print('    - [Info] The checkpoint file has been updated.')\n",
    "\n",
    "    all_epochs.append(epoch)\n",
    "\n",
    "    if early_stopping.early_stop(val_loss):\n",
    "        print(\"We are at epoch:\", epoch)\n",
    "        break\n",
    "\n",
    "# plot and save learning curve\n",
    "print('ALL DONE')               \n",
    "fig1 = plt.figure('Figure 1')\n",
    "plt.plot(train_losses, label = 'train')\n",
    "plt.plot(val_losses, label= 'valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim([0.0, 1])\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc =\"upper right\")\n",
    "plt.title('loss change curve')\n",
    "plt.savefig(f'checkpoint_best_loss_curve.png')\n",
    "\n",
    "fig2 = plt.figure('Figure 2')\n",
    "plt.plot(train_accs, label = 'train')\n",
    "plt.plot(val_accs, label= 'valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim([0.0, 1])\n",
    "plt.ylabel('acc')\n",
    "plt.legend(loc =\"upper right\")\n",
    "plt.title('accuracy change curve')\n",
    "plt.savefig(f'checkpoint_best_acc_curve.png')\n",
    "\n",
    "#load in best model and test\n",
    "chkpt = torch.load('checkpoint_best.pth', map_location='cuda')\n",
    "model.load_state_dict(chkpt['model'])\n",
    "test(test_loader, device, model, len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9eac04de96eedd62e00aed81546a88e65054f94b94fd2046011ecf2ba89c9a93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
