{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# read csv\n",
    "data = pd.read_csv('data.csv')\n",
    "data = data[0:5000]\n",
    "# observe first five instances of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe an example review to see if textual preprocessing is needed\n",
    "example_review = data.iloc[1, 0]\n",
    "example_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bs4 to access BeautifulSoup, which has a html parser\n",
    "from bs4 import BeautifulSoup\n",
    "# load re for substitute function\n",
    "import re\n",
    "\n",
    "def parse_review(text):\n",
    "    # functiion for parsing html string\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    parsed_text = soup.get_text()\n",
    "    return str(parsed_text)\n",
    "\n",
    "def clean_text(text):\n",
    "    # function for cleaning text\n",
    "\n",
    "    # parse text\n",
    "    parsed_text = parse_review(text)\n",
    "\n",
    "    # substitute symbology with an empty string\n",
    "    cleaned_text = re.sub(\"\\[[^]]*\\]'-\", '', parsed_text)\n",
    "\n",
    "    # lowercase all text\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whan/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# apply cleaning to the review column in data\n",
    "\n",
    "data['review'] = data['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn module for splitting data into 70:15:15 proportions and binarizing labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# X is our input and y is our target\n",
    "X = data['review'].values\n",
    "y = data['sentiment'].values\n",
    "\n",
    "# binarize labels\n",
    "binarized_labels = label_binarize(y, classes = ['negative', 'positive'])\n",
    "\n",
    "# reshape binarized labels array into a 1 dimensional array\n",
    "binarized_labels = binarized_labels.squeeze(1)\n",
    "\n",
    "\n",
    "# Split data into train validation and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, binarized_labels, shuffle = True, test_size=0.3, random_state=2)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, shuffle = True, test_size = 0.5, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    2532\n",
       "positive    2468\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how many positive and negative labels\n",
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227.1084\n"
     ]
    }
   ],
   "source": [
    "# get average token length\n",
    "\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    token_lengths.append(len(X[i].split()))\n",
    "    \n",
    "average_token_length = sum(token_lengths)/len(token_lengths)\n",
    "\n",
    "print(average_token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "max_length = 250\n",
    "batch_size = 16\n",
    "epochs = 250\n",
    "number_of_classes = 2\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "pretrained_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# garbage collect and empty cuda cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set device to gpu or cpu, whichever is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules to create PyTorch Dataset and PyTorch DataLoader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Create PyTorch Dataset class\n",
    "class TextSentimentDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments, tokenizer, max_length, device):\n",
    "        self.reviews = reviews\n",
    "        self.sentiments = sentiments\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentiments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        sentiment = self.sentiments[idx]\n",
    "        \n",
    "        # Tokenize reviews and retrieve input_ids for model\n",
    "        inputs = self.tokenizer([review], \n",
    "                                max_length = self.max_length,\n",
    "                                add_special_tokens = True,\n",
    "                                return_attention_mask = True, \n",
    "                                padding = 'max_length', \n",
    "                                truncation = True, \n",
    "                                return_tensors = 'pt').to(self.device)\n",
    "        input_ids = inputs.input_ids\n",
    "        attention_mask = inputs.attention_mask\n",
    "        \n",
    "        return input_ids, attention_mask, sentiment\n",
    "    \n",
    "\n",
    "def create_dataloader(X, y, batch_size, tokenizer, max_length, device):\n",
    "    # function for creating PyTorch DataLoader\n",
    "\n",
    "    pytorch_dataset = TextSentimentDataset(\n",
    "        reviews = X,\n",
    "        sentiments = y,\n",
    "        tokenizer = tokenizer,\n",
    "        max_length = max_length,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    pytorch_dataloader = DataLoader(\n",
    "        dataset = pytorch_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    return pytorch_dataloader, pytorch_dataset\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders for train, validation, test sets\n",
    "\n",
    "train_loader, train_dataset = create_dataloader(X_train, y_train, batch_size, tokenizer, max_length, device)\n",
    "val_loader, val_dataset = create_dataloader(X_val, y_val, batch_size, tokenizer, max_length, device)\n",
    "test_loader, test_dataset = create_dataloader(X_test, y_test, batch_size, tokenizer, max_length, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch modules for creating model and loss criterion\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def calculate_loss(pred, label):\n",
    "    #function for calculating loss and returning number of correct predictions\n",
    "\n",
    "    loss = F.cross_entropy(pred, label, reduction = 'sum')\n",
    "    pred = pred.max(1)[1]\n",
    "    number_correct = pred.eq(label).sum().item()\n",
    "\n",
    "    return loss, number_correct\n",
    "\n",
    "class SentimentAnalysis(nn.Module):\n",
    "    # Pytorch class for creating models\n",
    "    def __init__(self, number_of_classes, pretrained_model):\n",
    "        super(SentimentAnalysis, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        # Dropout for regularizing pretrained_model's outputs\n",
    "        self.dropout = nn.Dropout(p = 0.2)\n",
    "        # Linear layer to get prediction scores for each class\n",
    "        self.linear = nn.Linear(self.pretrained_model.config.hidden_size, number_of_classes)\n",
    "        # Softmax activation function to get probabilities \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.pretrained_model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "        last = out.last_hidden_state\n",
    "        pred = self.dropout(last)\n",
    "        pred = self.linear(pred)\n",
    "        pred = self.softmax(pred)\n",
    "        pred = pred[:, -1, :]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy module\n",
    "import numpy as np\n",
    "\n",
    "class EarlyStopper:\n",
    "    # stops the training process if the model does not improve validation loss\n",
    "    def __init__(self, patience, min_delta):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = SentimentAnalysis(\n",
    "    number_of_classes = number_of_classes,\n",
    "    pretrained_model = pretrained_model\n",
    ").to(device)\n",
    "\n",
    "# instantiate Adam optimizer\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#instantiate early stopping policy\n",
    "early_stopping = EarlyStopper(patience=3, min_delta=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def calculate_statistic(cm, class_num):\n",
    "    # function for calculating precision, recall, and f1 score from confusion matrix\n",
    "    total_pred = cm.sum(0)\n",
    "    total_true = cm.sum(1)\n",
    "\n",
    "    pre_i = [cm[i, i] / total_pred[i] for i in range(class_num)]\n",
    "    rec_i = [cm[i, i] / total_true[i] for i in range(class_num)]\n",
    "    F1_i = [2 * pre_i[i] * rec_i[i] / (pre_i[i] + rec_i[i]) for i in range(class_num)]\n",
    "\n",
    "    pre_i = np.array(pre_i)\n",
    "    rec_i = np.array(rec_i)\n",
    "    F1_i = np.array(F1_i)\n",
    "    pre_i[np.isnan(pre_i)] = 0\n",
    "    rec_i[np.isnan(rec_i)] = 0\n",
    "    F1_i[np.isnan(F1_i)] = 0\n",
    "\n",
    "    return sum(list(pre_i))/len(list(pre_i)), sum(list(rec_i))/len(list(rec_i)), sum(list(F1_i))/len(list(F1_i))\n",
    "\n",
    "\n",
    "# functions for training, evaluating, and testing\n",
    "\n",
    "def train(train_loader, device, model, optimizer, total_num):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0    \n",
    "    \n",
    "    for batch in tqdm(train_loader, desc='- (Training)  '): \n",
    "\n",
    "        input_ids, attention_mask, label = map(lambda x: x.to(device), batch)\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = input_ids.squeeze(1)\n",
    "        pred = model(input_ids, attention_mask)\n",
    "\n",
    "        loss, number_correct = calculate_loss(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += number_correct\n",
    "\n",
    "    train_loss = total_loss / total_num\n",
    "    train_acc = total_correct / total_num\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def validate(val_loader, device, model, total_num):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='- (Validating)  '): \n",
    "\n",
    "            input_ids, attention_mask, label = map(lambda x: x.to(device), batch)\n",
    "            input_ids = input_ids.squeeze(1)\n",
    "            pred = model(input_ids, attention_mask)\n",
    "            loss, number_correct = calculate_loss(pred, label)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += number_correct\n",
    "\n",
    "    val_loss = total_loss / total_num\n",
    "    val_acc = total_correct / total_num\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test(test_loader, device, model, total_num):\n",
    "    all_labels = []\n",
    "    all_res = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='- (Testing)  '): \n",
    "            \n",
    "            input_ids, attention_mask, label = map(lambda x: x.to(device), batch)\n",
    "            input_ids = input_ids.squeeze(1)\n",
    "            pred = model(input_ids, attention_mask)\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_res.extend(pred.max(1)[1].cpu().numpy())\n",
    "            loss, number_correct = calculate_loss(pred, label)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += number_correct\n",
    "\n",
    "    test_loss = total_loss / total_num\n",
    "    test_acc = total_correct / total_num\n",
    "\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Test Accuracy: {test_acc}')\n",
    "    \n",
    "    # create confusion matrix from labels and predictions\n",
    "    cm = confusion_matrix(all_labels, all_res)\n",
    "    print(f'Test confusion matrix: {cm}')\n",
    "    Precision, Recall, F1 = calculate_statistic(cm, number_of_classes)\n",
    "    print(f'Test Precision: {Precision}')\n",
    "    print(f'Test Recall: {Recall}')\n",
    "    print(f'Test F1: {F1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 0 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6932957098824637\n",
      "Train Accuracy: 0.49914285714285717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931738688151041\n",
      "Val Accuracy: 0.48533333333333334\n",
      "    - [Info] The checkpoint file has been updated.\n",
      "[ Epoch 1 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931448734828404\n",
      "Train Accuracy: 0.49742857142857144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931453119913737\n",
      "Val Accuracy: 0.496\n",
      "    - [Info] The checkpoint file has been updated.\n",
      "[ Epoch 2 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931626431601388\n",
      "Train Accuracy: 0.4942857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931423377990723\n",
      "Val Accuracy: 0.5053333333333333\n",
      "    - [Info] The checkpoint file has been updated.\n",
      "[ Epoch 3 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931562510899135\n",
      "Train Accuracy: 0.49228571428571427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931484616597493\n",
      "Val Accuracy: 0.4653333333333333\n",
      "[ Epoch 4 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:10<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931110303061349\n",
      "Train Accuracy: 0.5117142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931636136372884\n",
      "Val Accuracy: 0.4866666666666667\n",
      "[ Epoch 5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:08<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931780771527971\n",
      "Train Accuracy: 0.4937142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931452178955078\n",
      "Val Accuracy: 0.4826666666666667\n",
      "[ Epoch 6 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931323432922363\n",
      "Train Accuracy: 0.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931210009256998\n",
      "Val Accuracy: 0.5106666666666667\n",
      "    - [Info] The checkpoint file has been updated.\n",
      "[ Epoch 7 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931576058523996\n",
      "Train Accuracy: 0.5022857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931654688517253\n",
      "Val Accuracy: 0.4826666666666667\n",
      "[ Epoch 8 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:10<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931279798235213\n",
      "Train Accuracy: 0.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931551717122396\n",
      "Val Accuracy: 0.5013333333333333\n",
      "[ Epoch 9 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:10<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931770880562919\n",
      "Train Accuracy: 0.4888571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931063461303711\n",
      "Val Accuracy: 0.5146666666666667\n",
      "    - [Info] The checkpoint file has been updated.\n",
      "[ Epoch 10 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931624946594238\n",
      "Train Accuracy: 0.49228571428571427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931265131632487\n",
      "Val Accuracy: 0.5013333333333333\n",
      "[ Epoch 11 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6927909188951765\n",
      "Train Accuracy: 0.5028571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.693160893758138\n",
      "Val Accuracy: 0.49866666666666665\n",
      "[ Epoch 12 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:08<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931084123338972\n",
      "Train Accuracy: 0.49257142857142855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6932004216512044\n",
      "Val Accuracy: 0.4866666666666667\n",
      "[ Epoch 13 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931709700993129\n",
      "Train Accuracy: 0.4962857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931940816243489\n",
      "Val Accuracy: 0.4706666666666667\n",
      "[ Epoch 14 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:06<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931402830396379\n",
      "Train Accuracy: 0.49457142857142855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931202201843262\n",
      "Val Accuracy: 0.48533333333333334\n",
      "[ Epoch 15 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:08<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931498761858259\n",
      "Train Accuracy: 0.4977142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931790873209636\n",
      "Val Accuracy: 0.496\n",
      "[ Epoch 16 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931618870326451\n",
      "Train Accuracy: 0.49342857142857144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931635373433431\n",
      "Val Accuracy: 0.5066666666666667\n",
      "[ Epoch 17 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931644186292376\n",
      "Train Accuracy: 0.49457142857142855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  : 100%|██████████| 47/47 [00:07<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6931490999857585\n",
      "Val Accuracy: 0.504\n",
      "[ Epoch 18 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  : 100%|██████████| 219/219 [01:09<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6931389124734061\n",
      "Train Accuracy: 0.5085714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Validating)  :  57%|█████▋    | 27/47 [00:04<00:03,  6.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     19\u001b[0m train_accs\u001b[39m.\u001b[39mappend(train_acc)\n\u001b[0;32m---> 20\u001b[0m val_loss, val_acc \u001b[39m=\u001b[39m validate(val_loader, device, model, val_dataset\u001b[39m.\u001b[39;49m\u001b[39m__len__\u001b[39;49m())\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVal Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVal Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mval_acc\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 60\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(val_loader, device, model, total_num)\u001b[0m\n\u001b[1;32m     58\u001b[0m input_ids, attention_mask, label \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mto(device), batch)\n\u001b[1;32m     59\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m pred \u001b[39m=\u001b[39m model(input_ids, attention_mask)\n\u001b[1;32m     61\u001b[0m loss, number_correct \u001b[39m=\u001b[39m calculate_loss(pred, label)\n\u001b[1;32m     63\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[26], line 28\u001b[0m, in \u001b[0;36mSentimentAnalysis.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m---> 28\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrained_model(\n\u001b[1;32m     29\u001b[0m         input_ids \u001b[39m=\u001b[39;49m input_ids,\n\u001b[1;32m     30\u001b[0m         attention_mask \u001b[39m=\u001b[39;49m attention_mask\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m     last \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m     33\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(last)\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1021\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1012\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1014\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1015\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1016\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1020\u001b[0m )\n\u001b[0;32m-> 1021\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1022\u001b[0m     embedding_output,\n\u001b[1;32m   1023\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1024\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1025\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1026\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1027\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1028\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1029\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1030\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1031\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1032\u001b[0m )\n\u001b[1;32m   1033\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    611\u001b[0m         hidden_states,\n\u001b[1;32m    612\u001b[0m         attention_mask,\n\u001b[1;32m    613\u001b[0m         layer_head_mask,\n\u001b[1;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    616\u001b[0m         past_key_value,\n\u001b[1;32m    617\u001b[0m         output_attentions,\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:538\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    535\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    536\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 538\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    539\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    540\u001b[0m )\n\u001b[1;32m    541\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    543\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/transformers/pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 246\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    550\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 551\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    552\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:463\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 463\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import module for plotting learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_epoch = 0\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "all_epochs = []\n",
    "\n",
    "# training and validating loop\n",
    "for epoch in range(0, epochs):\n",
    "    print('[ Epoch', epoch, ']')\n",
    "\n",
    "    train_loss, train_acc = train(train_loader, device, model, optimizer, train_dataset.__len__())\n",
    "    print(f'Train Loss: {train_loss}')\n",
    "    print(f'Train Accuracy: {train_acc}')\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_loss, val_acc = validate(val_loader, device, model, val_dataset.__len__())\n",
    "    print(f'Val Loss: {val_loss}')\n",
    "    print(f'Val Accuracy: {val_acc}')\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # save best model checkpoint\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "    checkpoint = {\n",
    "        'model': model_state_dict,\n",
    "        'config_file': 'config',\n",
    "        'epoch': epoch}\n",
    "\n",
    "    if val_loss <= min(val_losses):\n",
    "        torch.save(checkpoint, 'checkpoint_best.pth')\n",
    "        print('    - [Info] The checkpoint file has been updated.')\n",
    "\n",
    "    all_epochs.append(epoch)\n",
    "\n",
    "    if early_stopping.early_stop(val_loss):\n",
    "        print(\"We are at epoch:\", epoch)\n",
    "        break\n",
    "\n",
    "# plot and save learning curve\n",
    "print('ALL DONE')               \n",
    "fig1 = plt.figure('Figure 1')\n",
    "plt.plot(train_losses, label = 'train')\n",
    "plt.plot(val_losses, label= 'valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim([0.0, 1])\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc =\"upper right\")\n",
    "plt.title('loss change curve')\n",
    "plt.savefig(f'checkpoint_best_loss_curve.png')\n",
    "\n",
    "fig2 = plt.figure('Figure 2')\n",
    "plt.plot(train_accs, label = 'train')\n",
    "plt.plot(val_accs, label= 'valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim([0.0, 1])\n",
    "plt.ylabel('acc')\n",
    "plt.legend(loc =\"upper right\")\n",
    "plt.title('accuracy change curve')\n",
    "plt.savefig(f'checkpoint_best_acc_curve.png')\n",
    "\n",
    "#load in best model and test\n",
    "chkpt = torch.load('checkpoint_best.pth', map_location='cuda')\n",
    "model.load_state_dict(chkpt['model'])\n",
    "test(test_loader, device, model, test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9eac04de96eedd62e00aed81546a88e65054f94b94fd2046011ecf2ba89c9a93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
