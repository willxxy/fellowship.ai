{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# read csv\n",
    "data = pd.read_csv('data.csv')\n",
    "data = data\n",
    "# observe first five instances of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe an example review to see if textual preprocessing is needed\n",
    "example_review = data.iloc[1, 0]\n",
    "example_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bs4 to access BeautifulSoup, which has a html parser\n",
    "from bs4 import BeautifulSoup\n",
    "# load re for substitute function\n",
    "import re\n",
    "\n",
    "def parse_review(text):\n",
    "    # functiion for parsing html string\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    parsed_text = soup.get_text()\n",
    "    return str(parsed_text)\n",
    "\n",
    "def clean_text(text):\n",
    "    # function for cleaning text\n",
    "\n",
    "    # parse text\n",
    "    parsed_text = parse_review(text)\n",
    "\n",
    "    # substitute symbology with an empty string\n",
    "    cleaned_text = re.sub(\"\\[[^]]*\\]'-\", '', parsed_text)\n",
    "\n",
    "    # lowercase all text\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whan/Desktop/fellowship.ai/.env/lib/python3.8/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# apply cleaning to the review column in data\n",
    "\n",
    "data['review'] = data['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn module for splitting data into 70:15:15 proportions and binarizing labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# X is our input and y is our target\n",
    "X = data['review'].values\n",
    "y = data['sentiment'].values\n",
    "\n",
    "# binarize labels\n",
    "binarized_labels = label_binarize(y, classes = ['negative', 'positive'])\n",
    "\n",
    "# reshape binarized labels array into a 1 dimensional array\n",
    "binarized_labels = binarized_labels.squeeze(1)\n",
    "\n",
    "\n",
    "# Split data into train validation and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, binarized_labels, shuffle = True, test_size=0.3, random_state=2)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, shuffle = True, test_size = 0.5, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how many positive and negative labels\n",
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227.11458\n"
     ]
    }
   ],
   "source": [
    "# get average token length\n",
    "\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    token_lengths.append(len(X[i].split()))\n",
    "    \n",
    "average_token_length = sum(token_lengths)/len(token_lengths)\n",
    "\n",
    "print(average_token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "max_length = 250\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "number_of_classes = 2\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "pretrained_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# garbage collect and empty cuda cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set device to gpu or cpu, whichever is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules to create PyTorch Dataset and PyTorch DataLoader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Create PyTorch Dataset class\n",
    "class TextSentimentDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments, tokenizer, max_length, device):\n",
    "        self.reviews = reviews\n",
    "        self.sentiments = sentiments\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentiments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        sentiment = self.sentiments[idx]\n",
    "        \n",
    "        # Tokenize reviews and retrieve input_ids for model\n",
    "        inputs = self.tokenizer(review, \n",
    "                                max_length = self.max_length,\n",
    "                                add_special_tokens = True,\n",
    "                                return_attention_mask = True, \n",
    "                                padding = 'max_length', \n",
    "                                truncation = True, \n",
    "                                return_tensors = 'pt').to(self.device)\n",
    "        input_ids = inputs.input_ids\n",
    "        attention_mask = inputs.attention_mask\n",
    "        sentiment = torch.tensor(sentiment, dtype=torch.long)\n",
    "        \n",
    "        return input_ids, attention_mask, sentiment\n",
    "    \n",
    "\n",
    "def create_dataloader(X, y, batch_size, tokenizer, max_length, device):\n",
    "    # function for creating PyTorch DataLoader\n",
    "\n",
    "    pytorch_dataset = TextSentimentDataset(\n",
    "        reviews = X,\n",
    "        sentiments = y,\n",
    "        tokenizer = tokenizer,\n",
    "        max_length = max_length,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    pytorch_dataloader = DataLoader(\n",
    "        dataset = pytorch_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    return pytorch_dataloader, pytorch_dataset\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders for train, validation, test sets\n",
    "\n",
    "train_loader, train_dataset = create_dataloader(X_train, y_train, batch_size, tokenizer, max_length, device)\n",
    "val_loader, val_dataset = create_dataloader(X_val, y_val, batch_size, tokenizer, max_length, device)\n",
    "test_loader, test_dataset = create_dataloader(X_test, y_test, batch_size, tokenizer, max_length, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch modules for creating model and loss criterion\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def calculate_loss(pred, label):\n",
    "    #function for calculating loss and returning number of correct predictions\n",
    "    loss = F.cross_entropy(pred, label, reduction = 'sum')\n",
    "    pred = pred.max(1)[1]\n",
    "    number_correct = pred.eq(label).sum().item()\n",
    "\n",
    "    return loss, number_correct\n",
    "\n",
    "class SentimentAnalysis(nn.Module):\n",
    "    # Pytorch class for creating models\n",
    "    def __init__(self, number_of_classes, pretrained_model):\n",
    "        super(SentimentAnalysis, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        # Softmax activation function to get probabilities \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.pretrained_model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "        last = out.logits\n",
    "        pred = self.softmax(last)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy module\n",
    "import numpy as np\n",
    "\n",
    "class EarlyStopper:\n",
    "    # stops the training process if the model does not improve validation loss\n",
    "    def __init__(self, patience, min_delta):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = SentimentAnalysis(\n",
    "    number_of_classes = number_of_classes,\n",
    "    pretrained_model = pretrained_model\n",
    ").to(device)\n",
    "\n",
    "# instantiate Adam optimizer\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#instantiate early stopping policy\n",
    "early_stopping = EarlyStopper(patience=3, min_delta=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def calculate_statistic(cm, class_num):\n",
    "    # function for calculating precision, recall, and f1 score from confusion matrix\n",
    "    total_pred = cm.sum(0)\n",
    "    total_true = cm.sum(1)\n",
    "\n",
    "    pre_i = [cm[i, i] / total_pred[i] for i in range(class_num)]\n",
    "    rec_i = [cm[i, i] / total_true[i] for i in range(class_num)]\n",
    "    F1_i = [2 * pre_i[i] * rec_i[i] / (pre_i[i] + rec_i[i]) for i in range(class_num)]\n",
    "\n",
    "    pre_i = np.array(pre_i)\n",
    "    rec_i = np.array(rec_i)\n",
    "    F1_i = np.array(F1_i)\n",
    "    pre_i[np.isnan(pre_i)] = 0\n",
    "    rec_i[np.isnan(rec_i)] = 0\n",
    "    F1_i[np.isnan(F1_i)] = 0\n",
    "\n",
    "    return sum(list(pre_i))/len(list(pre_i)), sum(list(rec_i))/len(list(rec_i)), sum(list(F1_i))/len(list(F1_i))\n",
    "\n",
    "\n",
    "# functions for training, evaluating, and testing\n",
    "\n",
    "def train(train_loader, device, model, optimizer, total_num):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0    \n",
    "    \n",
    "    for batch in tqdm(train_loader, desc='- (Training)  '): \n",
    "\n",
    "        input_ids, attention_mask, label = map(lambda x: x.to(device), batch)\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = input_ids.squeeze(1)\n",
    "        attention_mask = attention_mask.squeeze(1)\n",
    "        pred = model(input_ids, attention_mask)\n",
    "\n",
    "        loss, number_correct = calculate_loss(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_correct += number_correct\n",
    "\n",
    "    train_loss = total_loss / total_num\n",
    "    train_acc = total_correct / total_num\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def validate(val_loader, device, model, total_num):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='- (Validating)  '): \n",
    "\n",
    "            input_ids, attention_mask, label = map(lambda x: x.to(device), batch)\n",
    "            input_ids = input_ids.squeeze(1)\n",
    "            pred = model(input_ids, attention_mask)\n",
    "            loss, number_correct = calculate_loss(pred, label)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += number_correct\n",
    "\n",
    "    val_loss = total_loss / total_num\n",
    "    val_acc = total_correct / total_num\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test(test_loader, device, model, total_num):\n",
    "    all_labels = []\n",
    "    all_res = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='- (Testing)  '): \n",
    "            \n",
    "            input_ids, attention_mask, label = map(lambda x: x.to(device), batch)\n",
    "            input_ids = input_ids.squeeze(1)\n",
    "            pred = model(input_ids, attention_mask)\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_res.extend(pred.max(1)[1].cpu().numpy())\n",
    "            loss, number_correct = calculate_loss(pred, label)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += number_correct\n",
    "\n",
    "    test_loss = total_loss / total_num\n",
    "    test_acc = total_correct / total_num\n",
    "\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Test Accuracy: {test_acc}')\n",
    "    \n",
    "    # create confusion matrix from labels and predictions\n",
    "    cm = confusion_matrix(all_labels, all_res)\n",
    "    print(f'Test confusion matrix: {cm}')\n",
    "    Precision, Recall, F1 = calculate_statistic(cm, number_of_classes)\n",
    "    print(f'Test Precision: {Precision}')\n",
    "    print(f'Test Recall: {Recall}')\n",
    "    print(f'Test F1: {F1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 0 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- (Training)  :   2%|▏         | 33/2188 [00:10<11:21,  3.16it/s]"
     ]
    }
   ],
   "source": [
    "# import module for plotting learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_epoch = 0\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "all_epochs = []\n",
    "\n",
    "# training and validating loop\n",
    "for epoch in range(0, epochs):\n",
    "    print('[ Epoch', epoch, ']')\n",
    "\n",
    "    train_loss, train_acc = train(train_loader, device, model, optimizer, train_dataset.__len__())\n",
    "    print(f'Train Loss: {train_loss}')\n",
    "    print(f'Train Accuracy: {train_acc}')\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_loss, val_acc = validate(val_loader, device, model, val_dataset.__len__())\n",
    "    print(f'Val Loss: {val_loss}')\n",
    "    print(f'Val Accuracy: {val_acc}')\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # save best model checkpoint\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "    checkpoint = {\n",
    "        'model': model_state_dict,\n",
    "        'config_file': 'config',\n",
    "        'epoch': epoch}\n",
    "\n",
    "    if val_loss <= min(val_losses):\n",
    "        torch.save(checkpoint, 'checkpoint_best.pth')\n",
    "        print('    - [Info] The checkpoint file has been updated.')\n",
    "\n",
    "    all_epochs.append(epoch)\n",
    "\n",
    "    if early_stopping.early_stop(val_loss):\n",
    "        print(\"We are at epoch:\", epoch)\n",
    "        break\n",
    "\n",
    "# plot and save learning curve\n",
    "print('ALL DONE')               \n",
    "fig1 = plt.figure('Figure 1')\n",
    "plt.plot(train_losses, label = 'train')\n",
    "plt.plot(val_losses, label= 'valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim([0.0, 1])\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc =\"upper right\")\n",
    "plt.title('loss change curve')\n",
    "plt.savefig(f'checkpoint_best_loss_curve.png')\n",
    "\n",
    "fig2 = plt.figure('Figure 2')\n",
    "plt.plot(train_accs, label = 'train')\n",
    "plt.plot(val_accs, label= 'valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim([0.0, 1])\n",
    "plt.ylabel('acc')\n",
    "plt.legend(loc =\"upper right\")\n",
    "plt.title('accuracy change curve')\n",
    "plt.savefig(f'checkpoint_best_acc_curve.png')\n",
    "\n",
    "#load in best model and test\n",
    "chkpt = torch.load('checkpoint_best.pth', map_location='cuda')\n",
    "model.load_state_dict(chkpt['model'])\n",
    "test(test_loader, device, model, test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9eac04de96eedd62e00aed81546a88e65054f94b94fd2046011ecf2ba89c9a93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
